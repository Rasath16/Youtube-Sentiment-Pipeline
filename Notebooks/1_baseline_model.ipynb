{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1 Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Ha3O-_d__JN1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution ~ip (c:\\Users\\DELL\\anaconda3\\Lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~ip (c:\\Users\\DELL\\anaconda3\\Lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~ip (c:\\Users\\DELL\\anaconda3\\Lib\\site-packages)\n"
          ]
        }
      ],
      "source": [
        "%pip install -q mlflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QOJLJ57N4lso"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution ~ip (c:\\Users\\DELL\\anaconda3\\Lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~ip (c:\\Users\\DELL\\anaconda3\\Lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~ip (c:\\Users\\DELL\\anaconda3\\Lib\\site-packages)\n"
          ]
        }
      ],
      "source": [
        "%pip install -q boto3 awscli"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 Configure MLflow Tracking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "op6TubXQhPfw"
      },
      "outputs": [],
      "source": [
        "import mlflow\n",
        "\n",
        "# Set the remote tracking server URI\n",
        "mlflow.set_tracking_uri(\"http://ec2-54-211-18-166.compute-1.amazonaws.com:5000/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "PIyYuCwVhUeE"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Experiment: artifact_location='s3://mlfow-bucket-2025/424422613538872822', creation_time=1762870848089, experiment_id='424422613538872822', last_update_time=1762870848089, lifecycle_stage='active', name='RF Baseline Model 1', tags={'mlflow.experimentKind': 'custom_model_development'}>"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Set or create an experiment for the baseline model\n",
        "mlflow.set_experiment(\"RF Baseline Model 1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Loading, Cleaning, and Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 Import Libraries (Data & NLP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "gT5_m6-_Rrad"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "j_um_s1pc2Bo"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Download necessary NLTK data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Load Raw Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "u3hxNz36b6_7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>clean_comment</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>family mormon have never tried explain them t...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>buddhism has very much lot compatible with chr...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>seriously don say thing first all they won get...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>what you have learned yours and only yours wha...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>for your own benefit you may want read living ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                       clean_comment  category\n",
              "0   family mormon have never tried explain them t...         1\n",
              "1  buddhism has very much lot compatible with chr...         1\n",
              "2  seriously don say thing first all they won get...        -1\n",
              "3  what you have learned yours and only yours wha...         0\n",
              "4  for your own benefit you may want read living ...         1"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/Himanshu-1703/reddit-sentiment-analysis/refs/heads/main/data/reddit.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3 Initial Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "-Apko0jpb87p"
      },
      "outputs": [],
      "source": [
        "df.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "clean_comment    0\n",
            "category         0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(df.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "BIzRs9YZcAlR"
      },
      "outputs": [],
      "source": [
        "df.drop_duplicates(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "BDy25RHPcDQH"
      },
      "outputs": [],
      "source": [
        "df = df[~(df['clean_comment'].str.strip() == '')]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.4 Define and Apply Text Preprocessing Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "zsbpkHfIc39y"
      },
      "outputs": [],
      "source": [
        "# Define the preprocessing function\n",
        "def preprocess_comment(comment):\n",
        "    # Convert to lowercase\n",
        "    comment = comment.lower()\n",
        "\n",
        "    # Remove trailing and leading whitespaces\n",
        "    comment = comment.strip()\n",
        "\n",
        "    # Remove newline characters\n",
        "    comment = re.sub(r'\\n', ' ', comment)\n",
        "\n",
        "    # Remove non-alphanumeric characters, except punctuation\n",
        "    comment = re.sub(r'[^A-Za-z0-9\\s!?.,]', '', comment)\n",
        "\n",
        "    # Remove stopwords but retain important ones for sentiment analysis\n",
        "    stop_words = set(stopwords.words('english')) - {'not', 'but', 'however', 'no', 'yet'}\n",
        "    comment = ' '.join([word for word in comment.split() if word not in stop_words])\n",
        "\n",
        "    # Lemmatize the words\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    comment = ' '.join([lemmatizer.lemmatize(word) for word in comment.split()])\n",
        "\n",
        "    return comment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "3I0cY3nNc6wK"
      },
      "outputs": [],
      "source": [
        "# Apply the preprocessing function to the 'clean_comment' column\n",
        "df['clean_comment'] = df['clean_comment'].apply(preprocess_comment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "rQoUyz9Rc9Az"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>clean_comment</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>family mormon never tried explain still stare ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>buddhism much lot compatible christianity espe...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>seriously say thing first get complex explain ...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>learned want teach different focus goal not wr...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>benefit may want read living buddha living chr...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                       clean_comment  category\n",
              "0  family mormon never tried explain still stare ...         1\n",
              "1  buddhism much lot compatible christianity espe...         1\n",
              "2  seriously say thing first get complex explain ...        -1\n",
              "3  learned want teach different focus goal not wr...         0\n",
              "4  benefit may want read living buddha living chr...         1"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Feature Engineering and Data Split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Vectorization using CountVectorizer (Bag of Words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "EAKFpLNig7B2"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Vectorize the comments using Bag of Words (CountVectorizer)\n",
        "vectorizer = CountVectorizer(max_features=10000)  # Limiting features to 10000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "DEbBOYO9g-jq"
      },
      "outputs": [],
      "source": [
        "X = vectorizer.fit_transform(df['clean_comment']).toarray()\n",
        "y = df['category']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "SFtHOYDehF3A"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(36793, 10000)"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "efCnt68yhNs8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(36793,)"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "clean_comment    0\n",
            "category         0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(df.isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Train/Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "7CyCPT6IhWET"
      },
      "outputs": [],
      "source": [
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Model Training and MLflow Logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "GRF5A1ZCdBck"
      },
      "outputs": [],
      "source": [
        "import mlflow.sklearn\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "7CyCPT6IhWET"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/11/12 14:23:55 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üèÉ View run RandomForest_Baseline_TrainTestSplit at: http://ec2-54-211-18-166.compute-1.amazonaws.com:5000/#/experiments/424422613538872822/runs/b660663da9794d23bb503b4eac640d7f\n",
            "üß™ View experiment at: http://ec2-54-211-18-166.compute-1.amazonaws.com:5000/#/experiments/424422613538872822\n",
            "Accuracy: 0.6511754314444897\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import mlflow.sklearn\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Define and train a Random Forest baseline model and log results to MLflow\n",
        "with mlflow.start_run() as run:\n",
        "    # Log a description for the run\n",
        "    mlflow.set_tag(\"mlflow.runName\", \"RandomForest_Baseline_TrainTestSplit\")\n",
        "    mlflow.set_tag(\"experiment_type\", \"baseline\")\n",
        "    mlflow.set_tag(\"model_type\", \"RandomForestClassifier\")\n",
        "\n",
        "    # Add a description\n",
        "    mlflow.set_tag(\"description\", \"Baseline RandomForest model for sentiment analysis using Bag of Words (BoW) with a simple train-test split\")\n",
        "\n",
        "    # Log parameters for the vectorizer\n",
        "    mlflow.log_param(\"vectorizer_type\", \"CountVectorizer\")\n",
        "    mlflow.log_param(\"vectorizer_max_features\", vectorizer.max_features)\n",
        "\n",
        "    # Log Random Forest parameters\n",
        "    n_estimators = 200\n",
        "    max_depth = 15\n",
        "\n",
        "    mlflow.log_param(\"n_estimators\", n_estimators)\n",
        "    mlflow.log_param(\"max_depth\", max_depth)\n",
        "\n",
        "    # Initialize and train the model\n",
        "    model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions on the test set\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Calculate and log metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    mlflow.log_metric(\"accuracy\", accuracy)\n",
        "\n",
        "    classification_rep = classification_report(y_test, y_pred, output_dict=True)\n",
        "\n",
        "    for label, metrics in classification_rep.items():\n",
        "        if isinstance(metrics, dict):  # For precision, recall, f1-score, etc.\n",
        "            for metric, value in metrics.items():\n",
        "                mlflow.log_metric(f\"{label}_{metric}\", value)\n",
        "\n",
        "    # Confusion matrix plot\n",
        "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"Actual\")\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "\n",
        "    # Save and log the confusion matrix plot (use absolute path to avoid cwd issues)\n",
        "    confusion_path = os.path.abspath(\"confusion_matrix.png\")\n",
        "    plt.savefig(confusion_path)\n",
        "    plt.close()\n",
        "\n",
        "    if os.path.exists(confusion_path):\n",
        "        try:\n",
        "            mlflow.log_artifact(confusion_path)\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: failed to log confusion matrix artifact: {e}\")\n",
        "    else:\n",
        "        print(f\"Error: confusion matrix file not found at {confusion_path}\")\n",
        "\n",
        "    # Log the Random Forest model\n",
        "    try:\n",
        "        mlflow.sklearn.log_model(model, name=\"random_forest_model\")\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: failed to log model: {e}\")\n",
        "\n",
        "    # Log the dataset artifact\n",
        "    dataset_path = os.path.abspath(\"dataset.csv\")\n",
        "    df.to_csv(dataset_path, index=False)\n",
        "\n",
        "    if os.path.exists(dataset_path):\n",
        "        try:\n",
        "            mlflow.log_artifact(dataset_path)\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: failed to log dataset artifact: {e}\")\n",
        "    else:\n",
        "        print(f\"Error: dataset file not found at {dataset_path}\")\n",
        "\n",
        "# Display final accuracy\n",
        "print(f\"Accuracy: {accuracy}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Evaluation and Artifacts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.1 Classification Report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "4dEHQmkMcbsF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       1.00      0.01      0.01      1650\n",
            "           0       0.68      0.82      0.75      2555\n",
            "           1       0.63      0.85      0.72      3154\n",
            "\n",
            "    accuracy                           0.65      7359\n",
            "   macro avg       0.77      0.56      0.49      7359\n",
            "weighted avg       0.73      0.65      0.57      7359\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2 Save Preprocessed Data for Next Experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "clean_comment    0\n",
            "category         0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(df.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(36793, 2)"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "3FJARjXcckS6"
      },
      "outputs": [],
      "source": [
        "# Save the cleaned and preprocessed DataFrame to a new CSV file\n",
        "df.to_csv('../data/reddit_preprocessing.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "SnJl6IRhi38R"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>clean_comment</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>family mormon never tried explain still stare ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>buddhism much lot compatible christianity espe...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>seriously say thing first get complex explain ...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>learned want teach different focus goal not wr...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>benefit may want read living buddha living chr...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                       clean_comment  category\n",
              "0  family mormon never tried explain still stare ...         1\n",
              "1  buddhism much lot compatible christianity espe...         1\n",
              "2  seriously say thing first get complex explain ...        -1\n",
              "3  learned want teach different focus goal not wr...         0\n",
              "4  benefit may want read living buddha living chr...         1"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Verify the saved file\n",
        "pd.read_csv('../data/reddit_preprocessing.csv').head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(36793, 2)"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "clean_comment    0\n",
            "category         0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(df.isnull().sum())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
