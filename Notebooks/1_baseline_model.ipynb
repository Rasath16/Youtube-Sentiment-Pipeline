{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1 Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "Ha3O-_d__JN1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution ~ip (c:\\Users\\DELL\\anaconda3\\Lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~ip (c:\\Users\\DELL\\anaconda3\\Lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~ip (c:\\Users\\DELL\\anaconda3\\Lib\\site-packages)\n"
          ]
        }
      ],
      "source": [
        "%pip install -q mlflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "QOJLJ57N4lso"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution ~ip (c:\\Users\\DELL\\anaconda3\\Lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~ip (c:\\Users\\DELL\\anaconda3\\Lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~ip (c:\\Users\\DELL\\anaconda3\\Lib\\site-packages)\n"
          ]
        }
      ],
      "source": [
        "%pip install -q boto3 awscli"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 Configure MLflow Tracking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "op6TubXQhPfw"
      },
      "outputs": [],
      "source": [
        "import mlflow\n",
        "\n",
        "# Set the remote tracking server URI\n",
        "mlflow.set_tracking_uri(\"http://ec2-54-211-18-166.compute-1.amazonaws.com:5000/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "PIyYuCwVhUeE"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/11/11 19:50:48 INFO mlflow.tracking.fluent: Experiment with name 'RF Baseline Model 1' does not exist. Creating a new experiment.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Experiment: artifact_location='s3://mlfow-bucket-2025/424422613538872822', creation_time=1762870848089, experiment_id='424422613538872822', last_update_time=1762870848089, lifecycle_stage='active', name='RF Baseline Model 1', tags={}>"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Set or create an experiment for the baseline model\n",
        "mlflow.set_experiment(\"RF Baseline Model 1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Loading, Cleaning, and Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 Import Libraries (Data & NLP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "gT5_m6-_Rrad"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "j_um_s1pc2Bo"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Download necessary NLTK data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Load Raw Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "u3hxNz36b6_7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>clean_comment</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>family mormon have never tried explain them t...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>buddhism has very much lot compatible with chr...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>seriously don say thing first all they won get...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>what you have learned yours and only yours wha...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>for your own benefit you may want read living ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                       clean_comment  category\n",
              "0   family mormon have never tried explain them t...         1\n",
              "1  buddhism has very much lot compatible with chr...         1\n",
              "2  seriously don say thing first all they won get...        -1\n",
              "3  what you have learned yours and only yours wha...         0\n",
              "4  for your own benefit you may want read living ...         1"
            ]
          },
          "execution_count": 76,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/Himanshu-1703/reddit-sentiment-analysis/refs/heads/main/data/reddit.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3 Initial Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "-Apko0jpb87p"
      },
      "outputs": [],
      "source": [
        "df.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "BIzRs9YZcAlR"
      },
      "outputs": [],
      "source": [
        "df.drop_duplicates(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "BDy25RHPcDQH"
      },
      "outputs": [],
      "source": [
        "df = df[~(df['clean_comment'].str.strip() == '')]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.4 Define and Apply Text Preprocessing Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "zsbpkHfIc39y"
      },
      "outputs": [],
      "source": [
        "# Define the preprocessing function\n",
        "def preprocess_comment(comment):\n",
        "    # Convert to lowercase\n",
        "    comment = comment.lower()\n",
        "\n",
        "    # Remove trailing and leading whitespaces\n",
        "    comment = comment.strip()\n",
        "\n",
        "    # Remove newline characters\n",
        "    comment = re.sub(r'\\n', ' ', comment)\n",
        "\n",
        "    # Remove non-alphanumeric characters, except punctuation\n",
        "    comment = re.sub(r'[^A-Za-z0-9\\s!?.,]', '', comment)\n",
        "\n",
        "    # Remove stopwords but retain important ones for sentiment analysis\n",
        "    stop_words = set(stopwords.words('english')) - {'not', 'but', 'however', 'no', 'yet'}\n",
        "    comment = ' '.join([word for word in comment.split() if word not in stop_words])\n",
        "\n",
        "    # Lemmatize the words\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    comment = ' '.join([lemmatizer.lemmatize(word) for word in comment.split()])\n",
        "\n",
        "    return comment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "3I0cY3nNc6wK"
      },
      "outputs": [],
      "source": [
        "# Apply the preprocessing function to the 'clean_comment' column\n",
        "df['clean_comment'] = df['clean_comment'].apply(preprocess_comment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "rQoUyz9Rc9Az"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>clean_comment</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>family mormon never tried explain still stare ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>buddhism much lot compatible christianity espe...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>seriously say thing first get complex explain ...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>learned want teach different focus goal not wr...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>benefit may want read living buddha living chr...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                       clean_comment  category\n",
              "0  family mormon never tried explain still stare ...         1\n",
              "1  buddhism much lot compatible christianity espe...         1\n",
              "2  seriously say thing first get complex explain ...        -1\n",
              "3  learned want teach different focus goal not wr...         0\n",
              "4  benefit may want read living buddha living chr...         1"
            ]
          },
          "execution_count": 82,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Feature Engineering and Data Split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Vectorization using CountVectorizer (Bag of Words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "EAKFpLNig7B2"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Vectorize the comments using Bag of Words (CountVectorizer)\n",
        "vectorizer = CountVectorizer(max_features=10000)  # Limiting features to 10000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "DEbBOYO9g-jq"
      },
      "outputs": [],
      "source": [
        "X = vectorizer.fit_transform(df['clean_comment']).toarray()\n",
        "y = df['category']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "SFtHOYDehF3A"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(36793, 10000)"
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "efCnt68yhNs8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(36793,)"
            ]
          },
          "execution_count": 86,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Train/Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "7CyCPT6IhWET"
      },
      "outputs": [],
      "source": [
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Model Training and MLflow Logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "GRF5A1ZCdBck"
      },
      "outputs": [],
      "source": [
        "import mlflow.sklearn\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "7CyCPT6IhWET"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9055d7c17e3c45ea8268bfe5b8a03ced",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸƒ View run RandomForest_Baseline_TrainTestSplit at: http://ec2-54-211-18-166.compute-1.amazonaws.com:5000/#/experiments/424422613538872822/runs/d361e4a55fd3476cb25d59fc56467579\n",
            "ðŸ§ª View experiment at: http://ec2-54-211-18-166.compute-1.amazonaws.com:5000/#/experiments/424422613538872822\n",
            "Accuracy: 0.6511754314444897\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from mlflow.models import infer_signature\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# For CountVectorizer or TfidfVectorizer\n",
        "input_example = pd.DataFrame(\n",
        "    X_train[:1].toarray() if hasattr(X_train, \"toarray\") else X_train[:1],\n",
        "    columns=vectorizer.get_feature_names_out()\n",
        ")\n",
        "# Start MLflow run\n",
        "with mlflow.start_run() as run:\n",
        "    # -------------------\n",
        "    # Tags\n",
        "    # -------------------\n",
        "    mlflow.set_tags({\n",
        "        \"mlflow.runName\": \"RandomForest_Baseline_TrainTestSplit\",\n",
        "        \"experiment_type\": \"baseline\",\n",
        "        \"model_type\": \"RandomForestClassifier\",\n",
        "        \"description\": \"Baseline RandomForest model for sentiment analysis using Bag of Words (BoW) with a simple train-test split\"\n",
        "    })\n",
        "\n",
        "    # -------------------\n",
        "    # Log parameters\n",
        "    # -------------------\n",
        "    mlflow.log_param(\"vectorizer_type\", \"CountVectorizer\")\n",
        "    mlflow.log_param(\"vectorizer_max_features\", vectorizer.max_features)\n",
        "\n",
        "    n_estimators = 200\n",
        "    max_depth = 15\n",
        "    mlflow.log_param(\"n_estimators\", n_estimators)\n",
        "    mlflow.log_param(\"max_depth\", max_depth)\n",
        "\n",
        "    # -------------------\n",
        "    # Train model\n",
        "    # -------------------\n",
        "    model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # -------------------\n",
        "    # Make predictions and log metrics\n",
        "    # -------------------\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    mlflow.log_metric(\"accuracy\", accuracy)\n",
        "\n",
        "    classification_rep = classification_report(y_test, y_pred, output_dict=True)\n",
        "    for label, metrics in classification_rep.items():\n",
        "        if isinstance(metrics, dict):\n",
        "            for metric, value in metrics.items():\n",
        "                mlflow.log_metric(f\"{label}_{metric}\", value)\n",
        "\n",
        "    # -------------------\n",
        "    # Confusion matrix\n",
        "    # -------------------\n",
        "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"Actual\")\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "\n",
        "    confusion_path = os.path.abspath(\"confusion_matrix.png\")\n",
        "    plt.savefig(confusion_path)\n",
        "    plt.close()\n",
        "\n",
        "    if os.path.exists(confusion_path):\n",
        "        mlflow.log_artifact(confusion_path)\n",
        "\n",
        "    # -------------------\n",
        "    # Log model with signature and input example\n",
        "    # -------------------\n",
        "    signature = infer_signature(X_train, model.predict(X_train))\n",
        "    mlflow.sklearn.log_model(\n",
        "        model,\n",
        "        name=\"random_forest_model\",\n",
        "        signature=signature,\n",
        "        input_example=input_example\n",
        "    )\n",
        "\n",
        "    # -------------------\n",
        "    # Log dataset\n",
        "    # -------------------\n",
        "    dataset_path = os.path.abspath(\"dataset.csv\")\n",
        "    df.to_csv(dataset_path, index=False)\n",
        "    if os.path.exists(dataset_path):\n",
        "        mlflow.log_artifact(dataset_path)\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Evaluation and Artifacts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.1 Classification Report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "4dEHQmkMcbsF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       1.00      0.01      0.01      1650\n",
            "           0       0.68      0.82      0.75      2555\n",
            "           1       0.63      0.85      0.72      3154\n",
            "\n",
            "    accuracy                           0.65      7359\n",
            "   macro avg       0.77      0.56      0.49      7359\n",
            "weighted avg       0.73      0.65      0.57      7359\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2 Save Preprocessed Data for Next Experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "3FJARjXcckS6"
      },
      "outputs": [],
      "source": [
        "# Save the cleaned and preprocessed DataFrame to a new CSV file\n",
        "df.to_csv('../data/reddit_preprocessing.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "SnJl6IRhi38R"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>clean_comment</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>family mormon never tried explain still stare ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>buddhism much lot compatible christianity espe...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>seriously say thing first get complex explain ...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>learned want teach different focus goal not wr...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>benefit may want read living buddha living chr...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                       clean_comment  category\n",
              "0  family mormon never tried explain still stare ...         1\n",
              "1  buddhism much lot compatible christianity espe...         1\n",
              "2  seriously say thing first get complex explain ...        -1\n",
              "3  learned want teach different focus goal not wr...         0\n",
              "4  benefit may want read living buddha living chr...         1"
            ]
          },
          "execution_count": 93,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Verify the saved file\n",
        "pd.read_csv('../data/reddit_preprocessing.csv').head()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
